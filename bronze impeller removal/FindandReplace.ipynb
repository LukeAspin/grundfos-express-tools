{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Info is not in PSD format.\n",
      "Bom Breakdown is not in PSD format.\n",
      "Misc is not in PSD format.\n",
      "Seals is not in PSD format.\n",
      "Couplings is not in PSD format.\n"
     ]
    }
   ],
   "source": [
    "#Writing Replacement Function\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from utils.Dataframe_tools import PSD \n",
    "from utils import iterateSubDirFiles\n",
    "\n",
    "#Setting the Values\n",
    "IO='input files\\KPBom.xlsx'\n",
    "\n",
    "find_remove=[{\n",
    "    '91840054':'99717392',\n",
    "    '91840056':'99717485',\n",
    "    '98441969':'92758523',\n",
    "    '96698939':'92758528',     # in KPbom.xlsx \n",
    "    '96698968':'92758121',     # in KPbom.xlsx\n",
    "    '96768950':'92758767',     # in KPbom.xlsx\n",
    "    '99413027':'92758120',\n",
    "    '1020-3/4':'1020-5/6',\n",
    "    '1020-3_4':'1020-5_6',\n",
    "    'Desc-1020-3_4-KP_4':'Desc-1020-5_6-KP_6',\n",
    "    'Desc-1020-3_4-KP_3':'Desc-1020-5_6-KP_5'\n",
    "},]\n",
    "\n",
    "def find_and_replace(df:pd.DataFrame,to_replace,new_value)->None:\n",
    "    df.replace(to_replace=to_replace,value=new_value,regex=True,inplace=True)\n",
    "\n",
    "def change_psd_file(filename,function,*args,**kwargs)->list:\n",
    "    sheet_list=pd.ExcelFile(filename).sheet_names\n",
    "    df_dict={}\n",
    "    for sheet in sheet_list:\n",
    "        try:\n",
    "            df=PSD(io=filename,sheet_name=sheet)\n",
    "            function(df.psd_data,*args,**kwargs)\n",
    "            df_dict.update({sheet:df})\n",
    "        except TypeError:\n",
    "            print(sheet.title()+\" is not in PSD format.\")\n",
    "    return df_dict\n",
    "        \n",
    "\n",
    "dfs=change_psd_file(filename=IO,function=find_and_replace,to_replace=list(find_remove[0].keys()),new_value=list(find_remove[0].values()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "case=dfs['Case']\n",
    "data=case.psd_data\n",
    "header=case.header_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.Dataframe_tools import PSD\n",
    "filename='input files\\Copy of Price-master.xlsx'\n",
    "sheet='A'\n",
    "df=PSD(io=filename,sheet_name=sheet)\n",
    "data=df.psd_data\n",
    "header=df.header_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_row = data[data.iloc[:,0] == '[END]'].index.to_list()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "endr=header[header.iloc[:,0].str.startswith(\"[\")].index[0]\n",
    "classes=header[:endr].values.tolist()\n",
    "types=header.iloc[endr].tolist()\n",
    "arr=[]\n",
    "#Now I need to iterate over the classes\n",
    "for row in classes:\n",
    "    dct={}\n",
    "    dct2={}\n",
    "    new_data=data\n",
    "    new_data.columns=row\n",
    "    column_numbers = [x for x in range(new_data.shape[1])]\n",
    "    for j,col in enumerate(row):\n",
    "        if j==0:\n",
    "            dct.update({'class':col})\n",
    "        if str(col).strip().lower()=='nan':\n",
    "            column_numbers .remove(j)\n",
    "        else:\n",
    "            if j==0:\n",
    "                column_numbers.remove(0)\n",
    "            else:\n",
    "                dct2.update({col:types[j]})\n",
    "        if str(col).strip().lower()=='id':\n",
    "            data2=data.groupby(col)\n",
    "            IDs=list(data2.groups.keys())\n",
    "            dct.update({'instances':IDs})\n",
    "    new_data=new_data.iloc[:, column_numbers]\n",
    "    dct.update({'data':new_data})\n",
    "    dct.update({'DataTypes':dct2})\n",
    "    arr.append(dct)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lxml.etree as ET\n",
    "from typing import ParamSpec,Callable,Concatenate,TypeVar\n",
    "\n",
    "\n",
    "def create_control_node(class_name:str,column_name:str,pointer_merge_dict:dict={'merge':'true','position':\"end\",'offset':\"0\"})->ET.Element:\n",
    "    \"\"\"Function Description\"\"\"\n",
    "    #May need to make sure that the pointer merge dict contains valid entries\n",
    "    ControlNodeElement=ET.Element('controlnode') \n",
    "    classcontroldata=ET.SubElement(ControlNodeElement,'classcontroldata',attrib={'class':class_name})\n",
    "    attributecontroldata=ET.SubElement(classcontroldata,'attributecontroldata',{'attribute':column_name})\n",
    "    ET.SubElement(attributecontroldata,'pointermergedata',pointer_merge_dict)\n",
    "    return ControlNodeElement\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lxml.etree as ET\n",
    "\n",
    "\"\"\"\n",
    "Instances\n",
    "    |_Class\n",
    "        |_Instance\n",
    "            |_Class\n",
    "                |_Ref\n",
    "            |_Chartype\n",
    "            |_attribute (data)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"Imports: PSD data with header removed \"\"\"\n",
    "\n",
    "# num=1\n",
    "\n",
    "# #Outputs from my Code\n",
    "# psd_data=arr[num]['data'].iloc[:-1,:]\n",
    "# class_ref_name=arr[num]['class']\n",
    "# DataTypes=arr[num]['DataTypes']\n",
    "# Instances={x:ET.Element('instance',{'name':x}) for x in arr[num]['instances']} #List of elements that can appended to\n",
    "\n",
    "#setting up control node array \n",
    "cn_arr=[]\n",
    "instance_ele=ET.Element('instances')\n",
    "\n",
    "\n",
    "for row in arr:\n",
    "    #Constants for the row\n",
    "    class_ref_name=row['class']\n",
    "    psd_data=row['data'].iloc[:-1,:]\n",
    "    DataTypes=row['DataTypes']\n",
    "    Instances={x:ET.Element('instance',{'name':x}) for x in row['instances']}\n",
    "\n",
    "    # Iterate through dataframe in correct order to produce XML instance\n",
    "    root = ET.Element('class',{'name':class_ref_name})  # Root element\n",
    "\n",
    "    # Sets Class for each row/entry from PSD (Should be the same for all entries)\n",
    "    class_attr = {\"name\": class_ref_name}\n",
    "\n",
    "    # Sets chartype, which is always a doublebyte\n",
    "    chartype_attr = {\"value\": \"doublebyte\"}\n",
    "\n",
    "    for index, col in psd_data.iterrows():\n",
    "        #Need to use the instance assoicated with the row being looked at\n",
    "        instance=Instances[col['ID']]\n",
    "\n",
    "        #We need to check to see if the instance already exists in root\n",
    "        find_txt=f\"instance[@name='{instance.attrib['name']}']\"\n",
    "        if root.find(find_txt)==None:\n",
    "            root.append(instance)\n",
    "            # Inserts Class for each instance in PSD\n",
    "            class_entry = ET.SubElement(instance, \"class\")\n",
    "            class_sub_entry = ET.SubElement(class_entry, \"ref\", attrib=class_attr)\n",
    "            # Inserts chartype attribute\n",
    "            chartype = ET.SubElement(instance, \"chartype\", chartype_attr)\n",
    "\n",
    "        # # Loop through header dictionary for attribute types\n",
    "        for k, v in  DataTypes.items():\n",
    "            find_txt=f\"attribute[@name='{k}']\"\n",
    "            # Need to insert if statement where pointers receive sub-child\n",
    "            if k=='ID':\n",
    "                if 'pointer' in v:\n",
    "                    v='text'\n",
    "            else:\n",
    "                if v=='pointer-merge':\n",
    "                    v='pointer'\n",
    "                    try:\n",
    "                        next(item for item in cn_arr if item[\"class\"] == class_ref_name and item['attribute'] == k)\n",
    "                    except StopIteration:\n",
    "                        cn_arr.append({'element':create_control_node(class_ref_name,k),'class':class_ref_name,'attribute':k})\n",
    "            if v == \"text\":\n",
    "                attr_dict = {\"name\": k, \"type\": v, \"value\": psd_data.at[index, k].__str__()}\n",
    "                if instance.find(find_txt)==None:\n",
    "                    attr_entry = ET.SubElement(instance, \"attribute\", attr_dict) #This makes a new element each time and I want it to only make a new element\n",
    "                else:\n",
    "                    attr_entry=instance.find(find_txt) \n",
    "            elif \"pointer\" in str(v): # Whats the difference between pointer and pointer-merge? May have to change this. \n",
    "                attr_dict = {\"name\": k, \"type\": v}\n",
    "                if instance.find(find_txt)==None:\n",
    "                    attr_entry = ET.SubElement(instance, \"attribute\", attr_dict)\n",
    "                else:\n",
    "                    attr_entry=instance.find(find_txt)\n",
    "                pointer_dict = {\"value\": psd_data.at[index, k].__str__()}\n",
    "                if instance.find(f\"./attribute/ref[@value='{pointer_dict['value']}']\")==None:\n",
    "                    pointer_sub_entry = ET.SubElement(attr_entry, \"ref\", attrib=pointer_dict)\n",
    "            elif v == \"double\":\n",
    "                attr_dict = {\"name\": k, \"type\": v, \"value\": psd_data.at[index, k].__str__()}\n",
    "                if instance.find(find_txt)==None:\n",
    "                    attr_entry = ET.SubElement(instance, \"attribute\", attr_dict) #This makes a new element each time and I want it to only make a new element\n",
    "                else:\n",
    "                    attr_entry=instance.find(find_txt)\n",
    "    \n",
    "    instance_ele.append(root)\n",
    "\n",
    "\n",
    "root=ET.Element('FirepondSPCDatabase',attrib={'version':'3.0'}) #Creates the Root Node\n",
    "\n",
    "for node in cn_arr:\n",
    "    root.append(node['element'])\n",
    "\n",
    "root.append(instance_ele)\n",
    "\n",
    "with open('test.xml','wb') as doc:\n",
    "    doc.write(ET.tostring(root,encoding='UTF-8',xml_declaration=True,standalone=True,pretty_print=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xml.dom import minidom\n",
    "\n",
    "def prettify(elem):\n",
    "    \"\"\"Return a pretty-printed XML string for the Element.\n",
    "    \"\"\"\n",
    "    rough_string = ET.tostring(elem, 'UTF-8')\n",
    "    reparsed = minidom.parseString(rough_string)\n",
    "    print(reparsed.toprettyxml(indent=\"  \"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import DataFrame,Series\n",
    "from typing import Any,Union\n",
    "\n",
    "def _group_and_remove(groups,col_to_search,items_to_remove,df_list_to_remove,df_list_to_keep):\n",
    "    for _, frame in groups:\n",
    "        if any(frame[col_to_search].isin(items_to_remove)):   # Finding matching partnumbers to remove\n",
    "            # Need to add this sub-group to a \"removed dataframe\"\n",
    "            df_list_to_remove.append(frame)\n",
    "        else:\n",
    "            # Need to retain this sub-group, add to a \"keep dataframe\"\n",
    "            df_list_to_keep.append(frame)\n",
    "    return df_list_to_remove,df_list_to_keep\n",
    "\n",
    "\n",
    "\n",
    "def remove_from_df(df:DataFrame,cols_to_groupby:Union[str,list[str],None],col_to_search:Union[str,list[str]],items_to_remove:Union[list[Any],list[list[Any]]])->tuple[DataFrame, DataFrame]:\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "        cols_to_groupby : string or a list of strings\n",
    "            This represents the column(s) that the dataframe will be grouped into before searching the dataframe. \n",
    "            \n",
    "            >>> Example:\n",
    "              A B C\n",
    "            0 1 5 9\n",
    "            1 2 6 10\n",
    "            2 3 7 11\n",
    "            3 4 8 12\n",
    "            \n",
    "            If you get the cols_to_groupby to be a list containing ['A','B'] then the result would be 2 groups.\n",
    "\n",
    "            Result:\n",
    "            >>> Group A = [1 2 3 4]\n",
    "            >>> Group B = [5 6 7 8]\n",
    "\n",
    "            \"\"\"    \n",
    "    \n",
    "    df_list_to_remove = [] # will hold list of dataframes to be removed. Will concatenate to 1 dataframe later\n",
    "    df_list_to_keep   = [] # will hold list of dataframes to remain in PSD. Will concatenate to 1 dataframe later\n",
    "\n",
    "    #Lets check to see if the col_to_search is a list with more than one element or a string\n",
    "    if (isinstance(col_to_search,list) and isinstance(items_to_remove,list)) and (cols_to_groupby==None or isinstance(cols_to_groupby,str) or (isinstance(cols_to_groupby,list) and len(cols_to_groupby)==1)):\n",
    "        print('1')\n",
    "        if len(col_to_search)>1 and len(col_to_search)==len(items_to_remove) and isinstance(items_to_remove[0],list):\n",
    "            removals=df.copy()\n",
    "            keep=df.copy()\n",
    "            for i,col in enumerate(col_to_search):\n",
    "                df=df.astype({col:items_to_remove[i][0].__class__})\n",
    "                for item in items_to_remove[i]:\n",
    "                    removals=removals[removals[col]==item]\n",
    "                    keep=keep[keep[col]==item]\n",
    "            return removals,keep\n",
    "        elif len(col_to_search)==1:\n",
    "            removals=df.copy()\n",
    "            keep=df.copy()\n",
    "            for i,col in enumerate(col_to_search):\n",
    "                df=df.astype({col:items_to_remove[i][0].__class__})\n",
    "                for item in items_to_remove[i]:\n",
    "                    removals=removals[removals[col]==item]\n",
    "                    keep=keep[keep[col]==item]\n",
    "            return removals,keep\n",
    "    elif isinstance(col_to_search,str) and isinstance(items_to_remove,list):\n",
    "        print('2')\n",
    "        df=df.astype({col_to_search:items_to_remove[0].__class__})\n",
    "        groups = df.groupby(cols_to_groupby)\n",
    "        for _, frame in groups:\n",
    "            if any(frame[col_to_search].isin(items_to_remove)):   # Finding matching partnumbers to remove\n",
    "                # Need to add this sub-group to a \"removed dataframe\"\n",
    "                df_list_to_remove.append(frame)\n",
    "            else:\n",
    "                # Need to retain this sub-group, add to a \"keep dataframe\"\n",
    "                df_list_to_keep.append(frame)\n",
    "    elif not isinstance(items_to_remove,list):\n",
    "        raise ValueError(\"The items to remove must be in a list form if the columns to search by is not a list\")\n",
    "    # Concatenating list of dfs to single dfs.\n",
    "    if len(df_list_to_remove) > 0:\n",
    "        removals = pd.concat(df_list_to_remove)\n",
    "    else:\n",
    "        removals = pd.DataFrame(np.nan, index=[0], columns=psd_data.columns)\n",
    "    if len(df_list_to_keep)>0:\n",
    "        keep = pd.concat(df_list_to_keep)\n",
    "    else:\n",
    "        keep=pd.DataFrame(np.nan, index=[0], columns=psd_data.columns)\n",
    "    \n",
    "    return removals, keep\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "found=remove_from_df(arr[0]['data'],['Coating','CaseMaterial'],['Model'],[[':1020-5_6-KP:1020-5_6-KPVS:']])\n",
    "found1=found[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test=['1']\n",
    "isinstance(test[0],list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=['1','10']\n",
    "if test==None or isinstance(test,str) or isinstance(test,list):\n",
    "    if len(test)==1:\n",
    "        print('True')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5d89935b22884bac8e846ef6a5fb14ff565b7e382ac92ebe1031401d4b8e3f29"
  },
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
