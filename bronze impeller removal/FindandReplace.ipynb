{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Info is not in PSD format.\n",
      "Bom Breakdown is not in PSD format.\n",
      "Misc is not in PSD format.\n",
      "Seals is not in PSD format.\n",
      "Couplings is not in PSD format.\n"
     ]
    }
   ],
   "source": [
    "#Writing Replacement Function\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from utils.Dataframe_tools import PSD \n",
    "from utils import iterateSubDirFiles\n",
    "\n",
    "#Setting the Values\n",
    "IO='input files\\KPBom.xlsx'\n",
    "\n",
    "find_remove=[{\n",
    "    '91840054':'99717392',\n",
    "    '91840056':'99717485',\n",
    "    '98441969':'92758523',\n",
    "    '96698939':'92758528',     # in KPbom.xlsx \n",
    "    '96698968':'92758121',     # in KPbom.xlsx\n",
    "    '96768950':'92758767',     # in KPbom.xlsx\n",
    "    '99413027':'92758120',\n",
    "    '1020-3/4':'1020-5/6',\n",
    "    '1020-3_4':'1020-5_6',\n",
    "    'Desc-1020-3_4-KP_4':'Desc-1020-5_6-KP_6',\n",
    "    'Desc-1020-3_4-KP_3':'Desc-1020-5_6-KP_5'\n",
    "},]\n",
    "\n",
    "def find_and_replace(df:pd.DataFrame,to_replace,new_value)->None:\n",
    "    df.replace(to_replace=to_replace,value=new_value,regex=True,inplace=True)\n",
    "\n",
    "def change_psd_file(filename,function,*args,**kwargs)->list:\n",
    "    sheet_list=pd.ExcelFile(filename).sheet_names\n",
    "    df_dict={}\n",
    "    for sheet in sheet_list:\n",
    "        try:\n",
    "            df=PSD(io=filename,sheet_name=sheet)\n",
    "            function(df.psd_data,*args,**kwargs)\n",
    "            df_dict.update({sheet:df})\n",
    "        except TypeError:\n",
    "            print(sheet.title()+\" is not in PSD format.\")\n",
    "    return df_dict\n",
    "        \n",
    "\n",
    "dfs=change_psd_file(filename=IO,function=find_and_replace,to_replace=list(find_remove[0].keys()),new_value=list(find_remove[0].values()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "case=dfs['Case']\n",
    "data=case.psd_data\n",
    "header=case.header_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.Dataframe_tools import PSD\n",
    "filename='input files\\Copy of Price-master.xlsx'\n",
    "sheet='A'\n",
    "df=PSD(io=filename,sheet_name=sheet)\n",
    "data=df.psd_data\n",
    "header=df.header_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_row = data[data.iloc[:,0] == '[END]'].index.to_list()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "endr=header[header.iloc[:,0].str.startswith(\"[\")].index[0]\n",
    "classes=header[:endr].values.tolist()\n",
    "types=header.iloc[endr].tolist()\n",
    "arr=[]\n",
    "#Now I need to iterate over the classes\n",
    "for row in classes:\n",
    "    dct={}\n",
    "    dct2={}\n",
    "    new_data=data\n",
    "    new_data.columns=row\n",
    "    column_numbers = [x for x in range(new_data.shape[1])]\n",
    "    for j,col in enumerate(row):\n",
    "        if j==0:\n",
    "            dct.update({'class':col})\n",
    "        if str(col).strip().lower()=='nan':\n",
    "            column_numbers .remove(j)\n",
    "        else:\n",
    "            if j==0:\n",
    "                column_numbers.remove(0)\n",
    "            else:\n",
    "                dct2.update({col:types[j]})\n",
    "        if str(col).strip().lower()=='id':\n",
    "            data2=data.groupby(col)\n",
    "            IDs=list(data2.groups.keys())\n",
    "            dct.update({'instances':IDs})\n",
    "    new_data=new_data.iloc[:, column_numbers]\n",
    "    dct.update({'data':new_data})\n",
    "    dct.update({'DataTypes':dct2})\n",
    "    arr.append(dct)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lxml.etree as ET\n",
    "from typing import ParamSpec,Callable,Concatenate,TypeVar\n",
    "\n",
    "\n",
    "def create_control_node(class_name:str,column_name:str,pointer_merge_dict:dict={'merge':'true','position':\"end\",'offset':\"0\"})->ET.Element:\n",
    "    \"\"\"Function Description\"\"\"\n",
    "    #May need to make sure that the pointer merge dict contains valid entries\n",
    "    ControlNodeElement=ET.Element('controlnode') \n",
    "    classcontroldata=ET.SubElement(ControlNodeElement,'classcontroldata',attrib={'class':class_name})\n",
    "    attributecontroldata=ET.SubElement(classcontroldata,'attributecontroldata',{'attribute':column_name})\n",
    "    ET.SubElement(attributecontroldata,'pointermergedata',pointer_merge_dict)\n",
    "    return ControlNodeElement\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lxml.etree as ET\n",
    "\n",
    "\"\"\"\n",
    "Instances\n",
    "    |_Class\n",
    "        |_Instance\n",
    "            |_Class\n",
    "                |_Ref\n",
    "            |_Chartype\n",
    "            |_attribute (data)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"Imports: PSD data with header removed \"\"\"\n",
    "\n",
    "# num=1\n",
    "\n",
    "# #Outputs from my Code\n",
    "# psd_data=arr[num]['data'].iloc[:-1,:]\n",
    "# class_ref_name=arr[num]['class']\n",
    "# DataTypes=arr[num]['DataTypes']\n",
    "# Instances={x:ET.Element('instance',{'name':x}) for x in arr[num]['instances']} #List of elements that can appended to\n",
    "\n",
    "#setting up control node array \n",
    "cn_arr=[]\n",
    "instance_ele=ET.Element('instances')\n",
    "\n",
    "\n",
    "for row in arr:\n",
    "    #Constants for the row\n",
    "    class_ref_name=row['class']\n",
    "    psd_data=row['data'].iloc[:-1,:]\n",
    "    DataTypes=row['DataTypes']\n",
    "    Instances={x:ET.Element('instance',{'name':x}) for x in row['instances']}\n",
    "\n",
    "    # Iterate through dataframe in correct order to produce XML instance\n",
    "    root = ET.Element('class',{'name':class_ref_name})  # Root element\n",
    "\n",
    "    # Sets Class for each row/entry from PSD (Should be the same for all entries)\n",
    "    class_attr = {\"name\": class_ref_name}\n",
    "\n",
    "    # Sets chartype, which is always a doublebyte\n",
    "    chartype_attr = {\"value\": \"doublebyte\"}\n",
    "\n",
    "    for index, col in psd_data.iterrows():\n",
    "        #Need to use the instance assoicated with the row being looked at\n",
    "        instance=Instances[col['ID']]\n",
    "\n",
    "        #We need to check to see if the instance already exists in root\n",
    "        find_txt=f\"instance[@name='{instance.attrib['name']}']\"\n",
    "        if root.find(find_txt)==None:\n",
    "            root.append(instance)\n",
    "            # Inserts Class for each instance in PSD\n",
    "            class_entry = ET.SubElement(instance, \"class\")\n",
    "            class_sub_entry = ET.SubElement(class_entry, \"ref\", attrib=class_attr)\n",
    "            # Inserts chartype attribute\n",
    "            chartype = ET.SubElement(instance, \"chartype\", chartype_attr)\n",
    "\n",
    "        # # Loop through header dictionary for attribute types\n",
    "        for k, v in  DataTypes.items():\n",
    "            find_txt=f\"attribute[@name='{k}']\"\n",
    "            # Need to insert if statement where pointers receive sub-child\n",
    "            if k=='ID':\n",
    "                if 'pointer' in v:\n",
    "                    v='text'\n",
    "            else:\n",
    "                if v=='pointer-merge':\n",
    "                    v='pointer'\n",
    "                    try:\n",
    "                        next(item for item in cn_arr if item[\"class\"] == class_ref_name and item['attribute'] == k)\n",
    "                    except StopIteration:\n",
    "                        cn_arr.append({'element':create_control_node(class_ref_name,k),'class':class_ref_name,'attribute':k})\n",
    "            if v == \"text\":\n",
    "                attr_dict = {\"name\": k, \"type\": v, \"value\": psd_data.at[index, k].__str__()}\n",
    "                if instance.find(find_txt)==None:\n",
    "                    attr_entry = ET.SubElement(instance, \"attribute\", attr_dict) #This makes a new element each time and I want it to only make a new element\n",
    "                else:\n",
    "                    attr_entry=instance.find(find_txt) \n",
    "            elif \"pointer\" in str(v): # Whats the difference between pointer and pointer-merge? May have to change this. \n",
    "                attr_dict = {\"name\": k, \"type\": v}\n",
    "                if instance.find(find_txt)==None:\n",
    "                    attr_entry = ET.SubElement(instance, \"attribute\", attr_dict)\n",
    "                else:\n",
    "                    attr_entry=instance.find(find_txt)\n",
    "                pointer_dict = {\"value\": psd_data.at[index, k].__str__()}\n",
    "                if instance.find(f\"./attribute/ref[@value='{pointer_dict['value']}']\")==None:\n",
    "                    pointer_sub_entry = ET.SubElement(attr_entry, \"ref\", attrib=pointer_dict)\n",
    "            elif v == \"double\":\n",
    "                attr_dict = {\"name\": k, \"type\": v, \"value\": psd_data.at[index, k].__str__()}\n",
    "                if instance.find(find_txt)==None:\n",
    "                    attr_entry = ET.SubElement(instance, \"attribute\", attr_dict) #This makes a new element each time and I want it to only make a new element\n",
    "                else:\n",
    "                    attr_entry=instance.find(find_txt)\n",
    "    \n",
    "    instance_ele.append(root)\n",
    "\n",
    "\n",
    "root=ET.Element('FirepondSPCDatabase',attrib={'version':'3.0'}) #Creates the Root Node\n",
    "\n",
    "for node in cn_arr:\n",
    "    root.append(node['element'])\n",
    "\n",
    "root.append(instance_ele)\n",
    "\n",
    "with open('test.xml','wb') as doc:\n",
    "    doc.write(ET.tostring(root,encoding='UTF-8',xml_declaration=True,standalone=True,pretty_print=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xml.dom import minidom\n",
    "\n",
    "def prettify(elem):\n",
    "    \"\"\"Return a pretty-printed XML string for the Element.\n",
    "    \"\"\"\n",
    "    rough_string = ET.tostring(elem, 'UTF-8')\n",
    "    reparsed = minidom.parseString(rough_string)\n",
    "    print(reparsed.toprettyxml(indent=\"  \"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import DataFrame,Series\n",
    "from typing import Any\n",
    "\n",
    "def remove_from_df(df:DataFrame,cols_to_remove_from:list[str],items_to_remove:list[Any])->tuple[DataFrame, DataFrame]:\n",
    "    #Creating String List\n",
    "    str_list = [str(numstr) for numstr in items_to_remove]\n",
    "    \n",
    "    #Converts BOM column into the same datatype as the removal list\n",
    "    psd_data=df.astype({items_to_remove:str_list.__getitem__(0).__class__}) \n",
    "    \n",
    "    #Creating Groups for Grouping\n",
    "    groups = psd_data.groupby(cols_to_remove_from) \n",
    "    \n",
    "    df_list_to_remove = [] # will hold list of dataframes to be removed. Will concatenate to 1 dataframe later\n",
    "    df_list_to_keep   = [] # will hold list of dataframes to remain in PSD. Will concatenate to 1 dataframe later\n",
    "\n",
    "    # Iterates through each sub-group, checking if there is a pn that meets criteria for removal\n",
    "    for _, frame in groups:\n",
    "        if any(frame[items_to_remove].isin(str_list)):   # Finding matching partnumbers to remove\n",
    "            # Need to add this sub-group to a \"removed dataframe\"\n",
    "            df_list_to_remove.append(frame)\n",
    "        else:\n",
    "            # Need to retain this sub-group, add to a \"keep dataframe\"\n",
    "            df_list_to_keep.append(frame)\n",
    "    # Concatenating list of dfs to single dfs.\n",
    "    if len(df_list_to_remove) > 0:\n",
    "        removals = pd.concat(df_list_to_remove)\n",
    "    else:\n",
    "        removals = pd.DataFrame(np.nan, index=[0], columns=psd_data.columns)\n",
    "    keep = pd.concat(df_list_to_keep)\n",
    "    \n",
    "    return removals, keep\n",
    "\n",
    "def check_df(df:DataFrame,cols_to_search:list[str]=[\"ID\"],items_to_find:list=None)->tuple[bool,list]:\n",
    "    matches=remove_from_df(df,cols_to_search,items_to_find)\n",
    "    if matches[0].size>1:\n",
    "        return True,matches[0]['ID'].to_list()\n",
    "    elif matches[0]==1:\n",
    "        return False,[]\n",
    "    else:\n",
    "        raise Exception(f\"Can't have a dataframe with negative rows.\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5d89935b22884bac8e846ef6a5fb14ff565b7e382ac92ebe1031401d4b8e3f29"
  },
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
