{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "from openpyxl import load_workbook,worksheet,Workbook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MYDIR= r\"C:\\Projects\\2022\\Michaels_Code\\grundfos-express-tools\\bronze impeller removal\\input files\"\n",
    "OUTPUT_DIR=r\"C:\\Projects\\2022\\Michaels_Code\\grundfos-express-tools\\bronze impeller removal\\output files\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PNs to be Removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_pns = [\n",
    "    96699290, 97775274, 96699299, 97775277, 96778078,\n",
    "    97780992, 96699305, 96769184, 97778033, 96769190,\n",
    "    96769205, 97778039, 96769256, 96896891, 96769259,\n",
    "    96769271, 97780970, 96769280, 97780973\n",
    "]\n",
    "\n",
    "str_list = [str(numstr) for numstr in list_of_pns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Justification Note"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_removal_note():\n",
    "    timeStamp = datetime.datetime.now().strftime(\"%m-%d-%Y\")\n",
    "    reason = input(\"Reason for removal: \")\n",
    "    authority = input(\"Who authorized/requested this change? \")\n",
    "    return timeStamp + \" \" + reason + \" per \" + authority\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Geeting the First Row of the PSD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wb=load_workbook(r'C:\\Projects\\2022\\Michaels_Code\\grundfos-express-tools\\bronze impeller removal\\input files\\VL-VLSbom.xlsx',read_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_start_row(wb:Workbook,sheet_name:str,min_row:int=None,max_row:int=None,min_col:int=None,max_col:int=None):\n",
    "    try:\n",
    "        if not sheet_name in wb.sheetnames:\n",
    "            raise Exception\n",
    "        ws=wb[sheet_name]\n",
    "    except:\n",
    "        print('Sheet does not exist')\n",
    "    min_col = min_col or 1\n",
    "    min_row = min_row or 1\n",
    "    max_col = max_col or ws.max_column\n",
    "    max_row = max_row or ws.max_row\n",
    "    for row in ws.iter_rows(min_row,max_row,min_col,max_col):\n",
    "        if row[0].value=='[START]':\n",
    "            return row[0].row-1\n",
    "\n",
    "psd_startrow=get_start_row(wb,'Impeller',min_row=1,max_col=1)\n",
    "psd_startcol=0\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grouping Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_then_separate_by(psd_data:DataFrame,list_of_cols: list, pn_col: str)->tuple[DataFrame,DataFrame]:\n",
    "    \"\"\"list of cols are grouping categories. pn_col is the column that contains pns to find matches on.\"\"\"\n",
    "    groups = psd_data.groupby(list_of_cols) # Had to play around with this to get the right groupings\n",
    "\n",
    "    df_list_to_remove = [] # will hold list of dataframes to be removed. Will concatenate to 1 dataframe later\n",
    "    df_list_to_keep   = [] # will hold list of dataframes to remain in PSD. Will concatenate to 1 dataframe later\n",
    "\n",
    "    # Iterates through each sub-group, checking if there is a pn that meets criteria for removal\n",
    "    for _, frame in groups:\n",
    "        if any(frame[pn_col].isin(str_list)):   # Finding matching partnumbers to remove\n",
    "            df_list_to_remove.append(frame)     # Need to add this sub-group to a \"removed dataframe\"\n",
    "        else:\n",
    "            df_list_to_keep.append(frame)       # Need to retain this sub-group, add to a \"keep dataframe\"\n",
    "\n",
    "    # Concatenating list of dfs to single dfs.\n",
    "    removals = pd.concat(df_list_to_remove)\n",
    "    keep = pd.concat(df_list_to_keep)\n",
    "        \n",
    "    return removals, keep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterate Through Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from typing import Callable,Any,ParamSpec,TypeVar,Concatenate\n",
    "from utils.file_ops import read_files_in_dir\n",
    "\n",
    "\n",
    "P = ParamSpec('P')\n",
    "\n",
    "######## START HERE #####\n",
    "#Workbook contains Sheets and each file contains only one workbook therefore each file contains multiple sheets\n",
    "#We want to make a list for any given file of the sheets we want to process \n",
    "#We can take a single sheet and process over it with the below function\n",
    "\n",
    "\n",
    "\n",
    "files=read_files_in_dir(MYDIR)\n",
    "sheetname='Impeller'\n",
    "\n",
    "\n",
    "for file in files:\n",
    "    removal_note=add_removal_note()\n",
    "    data,fname=file\n",
    "    psd_data=pd.read_excel(data,sheet_name=sheetname,header=psd_startrow-1,dtype={'BOM':str})\n",
    "    original_psd_bottom_row = len(psd_data) + psd_startrow\n",
    "    \n",
    "    print(\"Opening file for updates: {}\".format(os.path.basename(fname)))\n",
    "    end_row = psd_data[psd_data['Full Data'] == '[END]'].index.to_list()[0]\n",
    "    psd_data = psd_data.iloc[:end_row]\n",
    "    group_by_columns = [\"Model\", \"Price ID\"]\n",
    "    removals, keep = group_then_separate_by(psd_data,group_by_columns, \"BOM\")\n",
    "    removals.loc[removals[\"Full Data\"] == \"[START]\", \"Full Data\"] = \"\"\n",
    "    new_row = pd.DataFrame({'ID': removal_note}, index =[0])\n",
    "    removals = pd.concat([new_row, removals[:]]).reset_index()\n",
    "    column_list = keep.columns\n",
    "    removals = removals[column_list]\n",
    "    removals.sort_values(by=['ID'], inplace=True)\n",
    "    keep.loc[0,'Full Data'] = np.nan\n",
    "    keep.sort_values(by=['ID'], inplace=True)\n",
    "    keep.reset_index(drop=True, inplace=True)\n",
    "    keep.loc[0,'Full Data'] = \"[START]\"\n",
    "    num_rows = len(keep)                      \n",
    "    keep.loc[num_rows,'Full Data']=\"[END]\"\n",
    "    append_location = original_psd_bottom_row - len(removals) \n",
    "    after_end_row = num_rows + psd_startrow + 2\n",
    "\n",
    "\n",
    "#Assuming only one sheet for right now\n",
    "# def before_change_data(dir,sheet_name:list[list[str]],is_same_note=True):\n",
    "#     files=read_files_in_dir(dir)\n",
    "#     for i,file in files:\n",
    "#         sheet=[]\n",
    "#         note=add_removal_note()\n",
    "#         for sheet in sheet_name[i]:\n",
    "#             sheet.append({'header':get_start_row(load_workbook(file[0],read_only=True),sheet_name=sheet,min_row=1,max_col=1)})\n",
    "#             if not is_same_note:\n",
    "#                 note=add_removal_note()\n",
    "            \n",
    "# before_change_data()\n",
    "\n",
    "\n",
    "# a=list(before_change_data(MYDIR))\n",
    "\n",
    "\n",
    "# def change_data(file:tuple[bytes,str,str],psd_data_func:Callable[Concatenate[P],DataFrame],psd_data_args:tuple[list,dict]):\n",
    "#     removal_note=add_removal_note()\n",
    "#     data,fname=file\n",
    "#     psd_data=psd_data_func(data,*psd_data_args[0]) #will have to be called as a list or all have to be the same\n",
    "#     original_psd_bottom_row = len(psd_data) + psd_startrow\n",
    "#     print(\"Opening file for updates: {}\".format(os.path.basename(fname)))\n",
    "#     #...This is where the real processing begins...\n",
    "#     end_row = psd_data[psd_data['Full Data'] == '[END]'].index.to_list()[0]\n",
    "#     psd_data = psd_data.iloc[:end_row]\n",
    "#     group_by_columns = [\"Model\", \"Price ID\"]\n",
    "#     removals, keep = group_then_separate_by(psd_data,group_by_columns, \"BOM\")\n",
    "#     removals.loc[removals[\"Full Data\"] == \"[START]\", \"Full Data\"] = np.nan\n",
    "#     new_row = pd.DataFrame({'ID': removal_note}, index =[0])\n",
    "#     removals = pd.concat([new_row, removals[:]]).reset_index()\n",
    "#     column_list = keep.columns\n",
    "#     removals = removals[column_list]\n",
    "#     removals.sort_values(by=['ID'], inplace=True)\n",
    "#     keep.loc[0,'Full Data'] = np.nan\n",
    "#     keep.sort_values(by=['ID'], inplace=True)\n",
    "#     keep.reset_index(drop=True, inplace=True)\n",
    "#     keep.loc[0,'Full Data'] = \"[START]\"\n",
    "#     return keep,removals,fname\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking for Misses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (len(keep[keep['BOM'].isin(str_list)])) == 0:\n",
    "    print(\"Didn't miss any partnumbers. Good to go\")\n",
    "else:\n",
    "    print(\"For some reason, the following entries were missed\")\n",
    "    print(keep[keep['BOM'].isin(str_list)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Return a formatted file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openpyxl.styles import PatternFill\n",
    "\n",
    "outputPath=r'C:\\Projects\\2022\\Michaels_Code\\grundfos-express-tools\\bronze impeller removal\\output files\\Revised Lbom-ES.xlsx'\n",
    "\n",
    "# wb = load_workbook(outputPath)\n",
    "# ws = wb['Impeller']\n",
    "# tabName = sheetname + \" No Bronze\"\n",
    "# wb.copy_worksheet(ws).title = tabName\n",
    "# ws_modified = wb[tabName]\n",
    "# ws_modified.delete_rows(after_end_row, len(removals)-1)\n",
    "# end_cell=ws.cell(row=after_end_row-1,column=1)\n",
    "# end_cell.fill=PatternFill(fill_type='solid',start_color=\"FFCC99\",end_color=\"FFCC99\")\n",
    "# for rows in ws_modified.iter_rows(min_row=append_location+2, max_row=append_location+2, min_col=1, max_col=40):\n",
    "#     for cell in rows:\n",
    "#         cell.fill = PatternFill(start_color=\"FF0000\", end_color=\"FF0000\", fill_type=\"solid\")\n",
    "# wb.save(outputPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outputPath=r'C:\\Projects\\2022\\Michaels_Code\\grundfos-express-tools\\bronze impeller removal\\output files\\Revised Lbom-ES.xlsx'\n",
    "# tabName = sheetname + \" No Bronze\"\n",
    "\n",
    "# with pd.ExcelWriter(outputPath, engine='openpyxl', mode='a', if_sheet_exists='overlay') as writer:  \n",
    "#             keep.to_excel(writer, sheet_name=tabName, index=False, startrow=psd_startrow-1, startcol=psd_startcol)\n",
    "#             removals.to_excel(writer, sheet_name=tabName, index=False, header=False, startrow=append_location+1, startcol=psd_startcol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df=pd.read_excel(outputPath,'Impeller',header=None)\n",
    "# header=df.loc[:4,:]\n",
    "# rest=df.loc[5:,:]\n",
    "# rest.reset_index(drop=True, inplace=True)\n",
    "# rest.columns=rest.loc[0,:]\n",
    "# rest=rest.drop(0)\n",
    "# from typing import overload,Literal,Sequence,Hashable,Iterable,Callable\n",
    "# from pandas._typing import DtypeArg,StorageOptions\n",
    "# from pandas.util._decorators import Appender\n",
    "\n",
    "\n",
    "outputPath=r'C:\\Projects\\2022\\Michaels_Code\\grundfos-express-tools\\bronze impeller removal\\output files\\Revised Lbom-ES.xlsx'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def _find_header_end(df:DataFrame)->int:\n",
    "#     for row in df.itertuples():\n",
    "#         if row[1]=='[START]':\n",
    "#             return int(row[0])\n",
    "\n",
    "\n",
    "# @overload\n",
    "# def _get_df(\n",
    "#     io,\n",
    "#     # sheet name is str or int -> DataFrame\n",
    "#     sheet_name: str | int,\n",
    "#     header: int | Sequence[int] | None = ...,\n",
    "#     names=...,\n",
    "#     index_col: int | Sequence[int] | None = ...,\n",
    "#     usecols=...,\n",
    "#     squeeze: bool | None = ...,\n",
    "#     dtype: DtypeArg | None = ...,\n",
    "#     engine: Literal[\"xlrd\", \"openpyxl\", \"odf\", \"pyxlsb\"] | None = ...,\n",
    "#     converters=...,\n",
    "#     true_values: Iterable[Hashable] | None = ...,\n",
    "#     false_values: Iterable[Hashable] | None = ...,\n",
    "#     skiprows: Sequence[int] | int | Callable[[int], object] | None = ...,\n",
    "#     nrows: int | None = ...,\n",
    "#     na_values=...,\n",
    "#     keep_default_na: bool = ...,\n",
    "#     na_filter: bool = ...,\n",
    "#     verbose: bool = ...,\n",
    "#     parse_dates=...,\n",
    "#     date_parser=...,\n",
    "#     thousands: str | None = ...,\n",
    "#     decimal: str = ...,\n",
    "#     comment: str | None = ...,\n",
    "#     skipfooter: int = ...,\n",
    "#     convert_float: bool | None = ...,\n",
    "#     mangle_dupe_cols: bool = ...,\n",
    "#     storage_options: StorageOptions = ...,\n",
    "# ) -> tuple[DataFrame,DataFrame,int]:\n",
    "#     ...\n",
    "\n",
    "\n",
    "# @Appender(pd.read_excel.__doc__,join='\\n')\n",
    "# def _get_df(**kwargs):\n",
    "#     \"\"\"This extends the pandas read_excel method for our uses.\n",
    "#     Params:\n",
    "#     **kwargs: Takes in keyword arguments only. These arguments also inherit from the \n",
    "#     \"\"\"\n",
    "#     df=pd.read_excel(**kwargs) #Reading the dataframe\n",
    "#     header_size=_find_header_end(df) #Getting the size of the header\n",
    "\n",
    "#     #Need to get the other dataframes\n",
    "#     psd_df=df.loc[header_size-1:,:]\n",
    "#     psd_df.reset_index(drop=True,inplace=True)\n",
    "#     psd_df.columns=psd_df.loc[0,:]\n",
    "#     psd_df=psd_df.drop(0)\n",
    "#     original_length=len(df)\n",
    "\n",
    "#     #Lets Check if we are using the header df\n",
    "#     if bool(kwargs.get('use_header',False)):\n",
    "#         if kwargs['use_header']==True:\n",
    "#             header=df.loc[:header_size-2,:]\n",
    "#             return header,psd_df,original_length\n",
    "#         else:\n",
    "#             return psd_df,original_length\n",
    "#     return psd_df,original_length\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process Sheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening file for updates: Revised Lbom-ES.xlsx\n",
      "Index(['Full Data', 'QP', 'ID', 'a', 'No Name 1', 'Model', 'CodeX', 'OptionID',\n",
      "       'Material', 'PACOMatlCode', 'Impeller Cap Screw and Washer',\n",
      "       'Impeller Key', 'Coating', 'BOM', 'Description', 'Price ID',\n",
      "       'LeadtimeID', 'Days', 'No Name 2', 'No Name 3', 'No Name 4',\n",
      "       'No Name 5', 'No Name 6'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\100477\\AppData\\Local\\Temp\\ipykernel_11860\\3022341470.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  psd_data[pn_col]=psd_data[pn_col].astype(str_list.__getitem__(0).__class__)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 694 6 760\n",
      "Closing file: Revised Lbom-ES.xlsx \n",
      " Wrote file: Revised Revised Lbom-ES.xlsx to C:\\Projects\\2022\\Michaels_Code\\grundfos-express-tools\\bronze impeller removal\\output files\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from _types._types import FilePath\n",
    "from pandas._typing import FilePath, ReadBuffer\n",
    "from typing import Union,ParamSpec,Callable\n",
    "from utils.file_ops import read_files_in_dir\n",
    "from utils.Dataframe_tools import PSD_BOM_Updates\n",
    "from pandas import DataFrame\n",
    "from openpyxl import load_workbook,worksheet,Workbook\n",
    "from openpyxl.styles.borders import Border,Side\n",
    "from openpyxl.styles import Alignment,PatternFill\n",
    "\n",
    "MYDIR= r\"C:\\Projects\\2022\\Michaels_Code\\grundfos-express-tools\\bronze impeller removal\\input files\"\n",
    "\n",
    "\n",
    "def add_removal_note():\n",
    "    timeStamp = datetime.datetime.now().strftime(\"%m-%d-%Y\")\n",
    "    reason = input(\"Reason for removal: \")\n",
    "    authority = input(\"Who authorized/requested this change? \")\n",
    "    return timeStamp + \" \" + reason + \" per \" + authority\n",
    "\n",
    "\n",
    "def group_then_separate_by(psd_data:DataFrame,list_of_cols: list, pn_col: str, str_list:list[str])->tuple[DataFrame,DataFrame]:\n",
    "    \"\"\"list of cols are grouping categories. pn_col is the column that contains pns to find matches on.\"\"\"\n",
    "    psd_data[pn_col]=psd_data[pn_col].astype(str_list.__getitem__(0).__class__)\n",
    "    groups = psd_data.groupby(list_of_cols) # Had to play around with this to get the right groupings\n",
    "\n",
    "    print(psd_data.columns)\n",
    "\n",
    "    df_list_to_remove = [] # will hold list of dataframes to be removed. Will concatenate to 1 dataframe later\n",
    "    df_list_to_keep   = [] # will hold list of dataframes to remain in PSD. Will concatenate to 1 dataframe later\n",
    "\n",
    "    # Iterates through each sub-group, checking if there is a pn that meets criteria for removal\n",
    "    for _, frame in groups:\n",
    "\n",
    "        if any(frame[pn_col].isin(str_list)):   # Finding matching partnumbers to remove\n",
    "            df_list_to_remove.append(frame)     # Need to add this sub-group to a \"removed dataframe\"\n",
    "        else:\n",
    "            df_list_to_keep.append(frame)       # Need to retain this sub-group, add to a \"keep dataframe\"\n",
    "\n",
    "    # Concatenating list of dfs to single dfs.\n",
    "    removals = pd.concat(df_list_to_remove)\n",
    "    keep = pd.concat(df_list_to_keep)\n",
    "        \n",
    "    return removals, keep\n",
    "\n",
    "#list of sheet lists can not be empty \n",
    "\n",
    "list_sheet_lists=[['Impeller']]\n",
    "target_dir=r'C:\\Projects\\2022\\Michaels_Code\\grundfos-express-tools\\bronze impeller removal\\output files'\n",
    "\n",
    "# list_of_pns = [\n",
    "#     96699290, 97775274, 96699299, 97775277, 96778078,\n",
    "#     97780992, 96699305, 96769184, 97778033, 96769190,\n",
    "#     96769205, 97778039, 96769256, 96896891, 96769259,\n",
    "#     96769271, 97780970, 96769280, 97780973\n",
    "# ]\n",
    "\n",
    "list_of_pns=[98876008,98876012]\n",
    "\n",
    "\n",
    "str_list = [str(numstr) for numstr in list_of_pns]\n",
    "\n",
    "\n",
    "def process_sheet(file: tuple[Union[FilePath, ReadBuffer[bytes], bytes],str],\n",
    "sheet_name: str | list[str],\n",
    "removal_note:str|None|bool=None,\n",
    "use_header:bool=False,\n",
    "**kwargs\n",
    ")->tuple[str,\n",
    "Union[tuple[DataFrame, int], tuple[DataFrame, DataFrame, int]],\n",
    "str,\n",
    "str]:\n",
    "    \"\"\"\n",
    "    Params:\n",
    "    data: Data comes from the binary of the file or the file path. \n",
    "    sheet_name: The name of the sheet to process.\n",
    "    removal_note: Does the sheet have its own removal note or does it share one with the entire workbook (file). The default is set to it shares a note.\"\"\"\n",
    "    #Check if sheetname is a valid sheet in the file\n",
    "    data,fname=file\n",
    "    try:\n",
    "        if not sheet_name in pd.ExcelFile(data).sheet_names:\n",
    "            raise ValueError(f\"Worksheet named '{sheet_name}' not found\")\n",
    "    except ValueError as err:\n",
    "        print(err)\n",
    "        return\n",
    "    #Need to get the header dataframe and the psd_dataframe\n",
    "    header,psd_data,length=PSD_BOM_Updates()._get_df(io=data,sheet_name=sheet_name,use_header=use_header,header=None)\n",
    "    #length = tuple of (psd_start_row,original size)\n",
    "    if not bool(removal_note):\n",
    "        removal_note=add_removal_note()\n",
    "    if header:\n",
    "        return fname,header,psd_data,length,removal_note,sheet_name\n",
    "    return fname,psd_data,length,removal_note,sheet_name\n",
    "\n",
    "    \n",
    "\n",
    "def process_file(file:FilePath,sheet_list:list[str],output_dir:str,one_removal_note:bool=False):\n",
    "    #Does this file use the same removal note for every sheet\n",
    "    if one_removal_note:\n",
    "        removal_note=add_removal_note()\n",
    "    else:\n",
    "        removal_note=None\n",
    "    if not isinstance(sheet_list,list):\n",
    "        raise Exception(f'You must give the sheets as a list to {process_file.__name__}')\n",
    "    output_file_name=os.path.join(output_dir,\"Revised \"+os.path.basename(file[1]))\n",
    "    ret=[]\n",
    "    for sheet in sheet_list:\n",
    "        val=list(process_sheet(file,sheet,removal_note))\n",
    "        val.extend([output_file_name])\n",
    "        ret.append(val)\n",
    "    return ret\n",
    "\n",
    "        \n",
    "\n",
    "def process_dir(dir,list_sheet_lists,output_dir:str,one_removal_note:bool):\n",
    "    files=read_files_in_dir(dir)\n",
    "    ret=[]\n",
    "    for i,file in enumerate(files):\n",
    "        if file[1].endswith(('.xls','.xlsx','.xlsm','.xlsb','.odf','.odt')):\n",
    "            ret.extend(process_file(file,list_sheet_lists[i],output_dir,one_removal_note))\n",
    "    return ret\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "grouping_func=(group_then_separate_by,[[\"Model\", \"Price ID\"] ,\"BOM\"],{'str_list':str_list})\n",
    "\n",
    "tasks=process_dir(MYDIR,[['ImpellerModified']],target_dir,True) #CHANGE HERE ##########################################################################\n",
    "\n",
    "def add_function(tasks,function,*args,**kwargs):\n",
    "    for task in tasks:\n",
    "        task.insert(2,(function,list(args),dict(kwargs)))\n",
    "    return tasks\n",
    "\n",
    "add_function(tasks,group_then_separate_by,[\"Model\", \"Price ID\"] ,\"BOM\",str_list=str_list)\n",
    "\n",
    "\n",
    "def write_new_PSD(file_name,\n",
    "psd_data,\n",
    "grouping_func:Callable[[ParamSpec.args],DataFrame],\n",
    "length:tuple[int,int],\n",
    "removal_note,\n",
    "sheet_name,\n",
    "outPut_file_path\n",
    "):\n",
    "    print(\"Opening file for updates: {}\".format(os.path.basename(file_name)))\n",
    "    psd_startrow=length[0]\n",
    "    psd_startcol=0\n",
    "    end_row = psd_data[psd_data['Full Data'] == '[END]'].index.to_list()[0] \n",
    "    psd_data = psd_data.iloc[:end_row]\n",
    "    removals, keep = grouping_func[0](psd_data,*grouping_func[1],**grouping_func[2])\n",
    "    removals.loc[removals[\"Full Data\"] == \"[START]\", \"Full Data\"] = \"\"\n",
    "    new_row = pd.DataFrame({'ID': removal_note}, index =[0])\n",
    "    removals = pd.concat([new_row, removals[:]]).reset_index()\n",
    "    column_list = keep.columns\n",
    "    removals = removals[column_list]\n",
    "    removals.sort_values(by=['ID'], inplace=True)\n",
    "    keep.loc[0,'Full Data'] = np.nan\n",
    "    keep.sort_values(by=['ID'], inplace=True)\n",
    "    keep.reset_index(drop=True, inplace=True)\n",
    "    keep.loc[0,'Full Data'] = \"[START]\"\n",
    "    num_rows = len(keep)                      \n",
    "    keep.loc[num_rows,'Full Data']=\"[END]\"\n",
    "    # append_location = length[1] - len(removals) #This is an issue\n",
    "    append_location=num_rows + psd_startrow\n",
    "    after_end_row = num_rows + psd_startrow + 2\n",
    "    wb = load_workbook(file_name)\n",
    "    ws = wb[sheet_name]\n",
    "    tabName = sheet_name + \"Modified\"\n",
    "    wb.copy_worksheet(ws).title = tabName\n",
    "    ws_modified = wb[tabName]\n",
    "    # ws_modified.delete_rows(after_end_row, len(removals)-1)\n",
    "    ws_modified.insert_rows(after_end_row-1,len(removals))\n",
    "    for item in ws_modified.iter_rows(min_col=1,max_col=len(removals.columns),min_row=after_end_row+2*len(removals)-2,max_row=after_end_row+2*len(removals)-2):\n",
    "        for cell in item:\n",
    "            cell.value=\"\"\n",
    "            cell.fill=PatternFill(fill_type=None)\n",
    "    end_cell = ws_modified.cell(row=after_end_row-1, column=1)\n",
    "    end_cell.fill=PatternFill(fill_type='solid',start_color=\"FFCC99\",end_color=\"FFCC99\")\n",
    "    for rows in ws_modified.iter_rows(min_row=append_location+2, max_row=append_location+2, min_col=1, max_col=40):\n",
    "        for cell in rows:\n",
    "            cell.fill = PatternFill(start_color=\"FF0000\", end_color=\"FF0000\", fill_type=\"solid\")\n",
    "    print(len(removals),num_rows,psd_startrow,length[1])\n",
    "    if len(removals)+num_rows+psd_startrow==length[1]:\n",
    "        print('Yes')\n",
    "        ws_modified.delete_rows(length[1], len(removals)-1)\n",
    "    wb.save(outPut_file_path)\n",
    "    del(ws,ws_modified,wb,end_cell)\n",
    "    with pd.ExcelWriter(outPut_file_path, engine='openpyxl', mode='a', if_sheet_exists='overlay') as writer:  \n",
    "            keep.to_excel(writer, sheet_name=tabName, index=False, startrow=psd_startrow-1, startcol=psd_startcol)\n",
    "            removals.to_excel(writer, sheet_name=tabName, index=False, header=False, startrow=append_location+1, startcol=psd_startcol)\n",
    "    print(\"Closing file: {}\".format(os.path.basename(file_name)),\"\\n\",f\"Wrote file: {os.path.basename(outPut_file_path)} to {os.path.dirname(outPut_file_path)}\")\n",
    "    \n",
    "\n",
    "for task in tasks:\n",
    "    write_new_PSD(*task)\n",
    "\n",
    "# write_new_PSD(*a[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making a New File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from _utils.excel_tools import write_excel_file\n",
    "from _utils.file_ops import add_str_to_filename\n",
    "from _utils.Dataframe_tools import write_df_to_excel\n",
    "\n",
    "header=process_sheet(file,'Impeller','No')\n",
    "tabName = sheetname + \" No Bronze\"\n",
    "\n",
    "\n",
    "with pd.ExcelWriter(outputPath, engine='openpyxl', mode='a', if_sheet_exists='overlay') as writer:\n",
    "        keep.to_excel(writer, sheet_name=tabName, index=False,startrow=psd_startrow-1,startcol=psd_startcol)\n",
    "        removals.to_excel(writer, sheet_name=tabName, index=False,header=False, startrow=append_location+1, startcol=psd_startcol)\n",
    "        header.to_excel(writer, sheet_name=tabName, index=False,header=False, startrow=0, startcol=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add Formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openpyxl.styles.borders import Border,Side\n",
    "from openpyxl.styles.fonts import Font\n",
    "from openpyxl.styles import Alignment,PatternFill\n",
    "from openpyxl import load_workbook,worksheet,Workbook\n",
    "from openpyxl.worksheet import cell_range\n",
    "\n",
    "\n",
    "outputPath=r'C:\\Projects\\2022\\Michaels_Code\\grundfos-express-tools\\bronze impeller removal\\output files\\Revised Lbom-ES.xlsx'\n",
    "sheetname='Impeller'\n",
    "tabName = sheetname + \" No Bronze\"\n",
    "psd_startrow=get_start_row(wb,'Impeller',min_row=1,max_col=1)\n",
    "\n",
    "\n",
    "wb=load_workbook(outputPath)\n",
    "ws=wb[tabName]\n",
    "\n",
    "#Adding the Border\n",
    "col=ws.column_dimensions['A']\n",
    "col.border=Border(right=Side(style='thick'))\n",
    "\n",
    "#Fitting the Cell Widths in the Columns\n",
    "dims = {}\n",
    "for row in ws.iter_rows(min_row=0):\n",
    "    for cell in row:\n",
    "        if cell.value:\n",
    "            dims[cell.column_letter] = max((dims.get(cell.column_letter, 0)+2, len(str(cell.value))+2))\n",
    "            if row[0].row <= psd_startrow and cell.column_letter=='A':\n",
    "                cell.border=Border(right=Side(style='thick'))\n",
    "            elif cell.row<psd_startrow and cell.value=='[END]': #Gets the column end\n",
    "                cell.fill=PatternFill(fill_type='solid',start_color=\"FFCC99\",end_color=\"FFCC99\")\n",
    "\n",
    "                #Applying Top Rows Formatting\n",
    "                for row2 in ws.iter_rows(min_col=0,max_col=cell.column-1,min_row=0,max_row=psd_startrow-1):\n",
    "                    for cell2 in row2:\n",
    "                        if cell2.row==1:\n",
    "                            cell2.fill=PatternFill(fill_type='solid',start_color=\"808080\",end_color=\"808080\")\n",
    "                            cell2.font=Font(color='00FFFFFF')\n",
    "                            if cell2.column==1:\n",
    "                                cell2.font=Font(bold=True,color='00FFFFFF')\n",
    "                                cell2.border=Border(right=Side(style='thick'),bottom=Side(style='thick'))\n",
    "                            else:\n",
    "                                cell2.border=Border(bottom=Side(style='thick'))\n",
    "                        elif cell2.row==cell.row-1:\n",
    "                            cell2.fill=PatternFill(fill_type='solid',start_color=\"FFCC99\",end_color=\"FFCC99\")\n",
    "                            if cell2.column==1:\n",
    "                                cell2.border=Border(right=Side(style='thick'),bottom=Side(style='thin'))\n",
    "                            else:\n",
    "                                cell2.border=Border(bottom=Side(style='thin'))\n",
    "                        elif cell2.row==psd_startrow-1:\n",
    "                            cell2.fill=PatternFill(fill_type='solid',start_color=\"FFCC99\",end_color=\"FFCC99\")\n",
    "                            if cell2.column==1:\n",
    "                                cell2.border=Border(right=Side(style='thick'),bottom=Side(style='thick'))\n",
    "                            else:\n",
    "                                cell2.border=Border(bottom=Side(style='thick'))\n",
    "                        else:\n",
    "                            cell2.fill=PatternFill(fill_type='solid',start_color=\"FFCC99\",end_color=\"FFCC99\")\n",
    "                        if cell2.column>1:\n",
    "                            cell2.alignment=Alignment(horizontal='center')\n",
    "                        else:\n",
    "                            cell2.alignment=Alignment(horizontal='left')\n",
    "                flt=str(cell_range.CellRange(min_col=2, min_row=psd_startrow, max_col=cell.column-1, max_row=psd_startrow))\n",
    "                ws.auto_filter.ref=flt     \n",
    "            elif cell.row>psd_startrow and cell.column==1 and cell.value=='[START]':\n",
    "                cell.fill=PatternFill(fill_type='solid',start_color=\"FFCC99\",end_color=\"FFCC99\")\n",
    "                cell.alignment=Alignment(horizontal='right')\n",
    "            \n",
    "            elif cell.value=='[END]' and cell.row>psd_startrow and cell.column==1:\n",
    "                cell.fill=PatternFill(fill_type='solid',start_color=\"FFCC99\",end_color=\"FFCC99\")\n",
    "            \n",
    "            elif cell.row==append_location+2 and cell.column>=1 and cell.column<=40:\n",
    "                cell.fill = PatternFill(start_color=\"FF0000\", end_color=\"FF0000\", fill_type=\"solid\")\n",
    "                                   \n",
    "for col, value in dims.items():\n",
    "    ws.column_dimensions[col].width = value\n",
    "\n",
    "\n",
    "wb.save(outputPath)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- it is faster to just use the copy code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psd_data=tasks[1][1]\n",
    "psd_data\n",
    "\n",
    "a,b=group_then_separate_by(psd_data,[\"Model\", \"Price ID\"],'BOM',str_list)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5d89935b22884bac8e846ef6a5fb14ff565b7e382ac92ebe1031401d4b8e3f29"
  },
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
