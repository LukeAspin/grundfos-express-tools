{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "from openpyxl import load_workbook,worksheet,Workbook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MYDIR= r\"C:\\Projects\\2022\\Michaels_Code\\grundfos-express-tools\\bronze impeller removal\\input files\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PNs to be Removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_pns = [\n",
    "    96699290, 97775274, 96699299, 97775277, 96778078,\n",
    "    97780992, 96699305, 96769184, 97778033, 96769190,\n",
    "    96769205, 97778039, 96769256, 96896891, 96769259,\n",
    "    96769271, 97780970, 96769280, 97780973\n",
    "]\n",
    "\n",
    "str_list = [str(numstr) for numstr in list_of_pns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Justification Note"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_removal_note():\n",
    "    timeStamp = datetime.datetime.now().strftime(\"%m-%d-%Y\")\n",
    "    reason = input(\"Reason for removal: \")\n",
    "    authority = input(\"Who authorized/requested this change? \")\n",
    "    return timeStamp + \" \" + reason + \" per \" + authority\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Geeting the First Row of the PSD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "wb=load_workbook(r'C:\\Projects\\2022\\Michaels_Code\\grundfos-express-tools\\bronze impeller removal\\input files\\Lbom-ES.xlsx',read_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_start_row(wb:Workbook,sheet_name:str,min_row:int=None,max_row:int=None,min_col:int=None,max_col:int=None):\n",
    "    try:\n",
    "        if not sheet_name in wb.sheetnames:\n",
    "            raise Exception\n",
    "        ws=wb[sheet_name]\n",
    "    except:\n",
    "        print('Sheet does not exist')\n",
    "    min_col = min_col or 1\n",
    "    min_row = min_row or 1\n",
    "    max_col = max_col or ws.max_column\n",
    "    max_row = max_row or ws.max_row\n",
    "    for row in ws.iter_rows(min_row,max_row,min_col,max_col):\n",
    "        if row[0].value=='[START]':\n",
    "            return row[0].row-1\n",
    "\n",
    "psd_startrow=get_start_row(wb,'Impeller',min_row=1,max_col=1)\n",
    "psd_startcol=0\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grouping Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_then_separate_by(psd_data:DataFrame,list_of_cols: list, pn_col: str)->tuple[DataFrame,DataFrame]:\n",
    "    \"\"\"list of cols are grouping categories. pn_col is the column that contains pns to find matches on.\"\"\"\n",
    "    groups = psd_data.groupby(list_of_cols) # Had to play around with this to get the right groupings\n",
    "\n",
    "    df_list_to_remove = [] # will hold list of dataframes to be removed. Will concatenate to 1 dataframe later\n",
    "    df_list_to_keep   = [] # will hold list of dataframes to remain in PSD. Will concatenate to 1 dataframe later\n",
    "\n",
    "    # Iterates through each sub-group, checking if there is a pn that meets criteria for removal\n",
    "    for _, frame in groups:\n",
    "        if any(frame[pn_col].isin(str_list)):   # Finding matching partnumbers to remove\n",
    "            df_list_to_remove.append(frame)     # Need to add this sub-group to a \"removed dataframe\"\n",
    "        else:\n",
    "            df_list_to_keep.append(frame)       # Need to retain this sub-group, add to a \"keep dataframe\"\n",
    "\n",
    "    # Concatenating list of dfs to single dfs.\n",
    "    removals = pd.concat(df_list_to_remove)\n",
    "    keep = pd.concat(df_list_to_keep)\n",
    "        \n",
    "    return removals, keep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterate Through Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from typing import Callable,Any,ParamSpec,TypeVar,Concatenate\n",
    "from _utils.file_ops import read_files_in_dir\n",
    "from _utils.Dataframe_tools import get_df\n",
    "\n",
    "P = ParamSpec('P')\n",
    "\n",
    "\n",
    "\n",
    "files=read_files_in_dir(MYDIR)\n",
    "sheetname='Impeller'\n",
    "\n",
    "\n",
    "# for file in files:\n",
    "#     removal_note=add_removal_note()\n",
    "#     data,fname=file\n",
    "#     psd_data=pd.read_excel(data,sheet_name=sheetname,header=psd_startrow-1,dtype={'BOM':str})\n",
    "#     original_psd_bottom_row = len(psd_data) + psd_startrow\n",
    "#     print(\"Opening file for updates: {}\".format(os.path.basename(fname)))\n",
    "#     end_row = psd_data[psd_data['Full Data'] == '[END]'].index.to_list()[0]\n",
    "#     psd_data = psd_data.iloc[:end_row]\n",
    "#     group_by_columns = [\"Model\", \"Price ID\"]\n",
    "#     removals, keep = group_then_separate_by(psd_data,group_by_columns, \"BOM\")\n",
    "#     removals.loc[removals[\"Full Data\"] == \"[START]\", \"Full Data\"] = \"\"\n",
    "#     new_row = pd.DataFrame({'ID': removal_note}, index =[0])\n",
    "#     removals = pd.concat([new_row, removals[:]]).reset_index()\n",
    "#     column_list = keep.columns\n",
    "#     removals = removals[column_list]\n",
    "#     removals.sort_values(by=['ID'], inplace=True)\n",
    "#     keep.loc[0,'Full Data'] = np.nan\n",
    "#     keep.sort_values(by=['ID'], inplace=True)\n",
    "#     keep.reset_index(drop=True, inplace=True)\n",
    "#     keep.loc[0,'Full Data'] = \"[START]\"\n",
    "\n",
    "#Assuming only one sheet for right now\n",
    "def before_change_data(dir,sheet_name:list[list[str]],is_same_note=True):\n",
    "    files=read_files_in_dir(dir)\n",
    "    for i,file in files:\n",
    "        header=[]\n",
    "        note=add_removal_note()\n",
    "        for sheet in sheet_name[i]:\n",
    "            header.append(get_start_row(load_workbook(file[0],read_only=True),sheet_name=sheet,min_row=1,max_col=1))\n",
    "            if not is_same_note:\n",
    "                note=add_removal_note()\n",
    "            \n",
    "\n",
    "    # if is_same_note:\n",
    "    #     note=add_removal_note()\n",
    "    #     return ([file[0],file[1],note] for file in files) \n",
    "    # else:\n",
    "    #     return ([file[0],file[1],add_removal_note()] for file in files)\n",
    "\n",
    "\n",
    "\n",
    "# a=list(before_change_data(MYDIR))\n",
    "\n",
    "\"\"\"Challenges are \"\"\"\n",
    "\n",
    "def change_data(file:tuple[bytes,str,str],psd_data_func:Callable[Concatenate[P],DataFrame],psd_data_args:tuple[list,dict]):\n",
    "    removal_note=add_removal_note()\n",
    "    data,fname=file\n",
    "    psd_data=psd_data_func(data,*psd_data_args[0],**psd_data_args[1]) #will have to be called as a list or all have to be the same\n",
    "    original_psd_bottom_row = len(psd_data) + psd_startrow\n",
    "    print(\"Opening file for updates: {}\".format(os.path.basename(fname)))\n",
    "    end_row = psd_data[psd_data['Full Data'] == '[END]'].index.to_list()[0]\n",
    "    psd_data = psd_data.iloc[:end_row]\n",
    "    group_by_columns = [\"Model\", \"Price ID\"]\n",
    "    removals, keep = group_then_separate_by(psd_data,group_by_columns, \"BOM\")\n",
    "    removals.loc[removals[\"Full Data\"] == \"[START]\", \"Full Data\"] = np.nan\n",
    "    new_row = pd.DataFrame({'ID': removal_note}, index =[0])\n",
    "    removals = pd.concat([new_row, removals[:]]).reset_index()\n",
    "    column_list = keep.columns\n",
    "    removals = removals[column_list]\n",
    "    removals.sort_values(by=['ID'], inplace=True)\n",
    "    keep.loc[0,'Full Data'] = np.nan\n",
    "    keep.sort_values(by=['ID'], inplace=True)\n",
    "    keep.reset_index(drop=True, inplace=True)\n",
    "    keep.loc[0,'Full Data'] = \"[START]\"\n",
    "    return keep,removals,fname\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding in Multi-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can't be done in this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main process blocking...\n"
     ]
    }
   ],
   "source": [
    "from time import sleep\n",
    "from random import random\n",
    "from multiprocessing import Process\n",
    "from multiprocessing import Event\n",
    " \n",
    "# target task function\n",
    "def task(event, number):\n",
    "    # wait for the event to be set\n",
    "    print(f'Process {number} waiting...', flush=True)\n",
    "    event.wait()\n",
    "    # begin processing\n",
    "    value = random()\n",
    "    sleep(value)\n",
    "    print(f'Process {number} got {value}', flush=True)\n",
    " \n",
    "# entry point\n",
    "if __name__ == '__main__':\n",
    "    # create a shared event object\n",
    "    event = Event()\n",
    "    # create a suite of processes\n",
    "    processes = [Process(target=task, args=(event, i)) for i in range(5)]\n",
    "    # start all processes\n",
    "    for process in processes:\n",
    "        process.start()\n",
    "    # block for a moment\n",
    "    print('Main process blocking...')\n",
    "    sleep(2)\n",
    "    # trigger all child processes\n",
    "    event.set()\n",
    "    # wait for all child processes to terminate\n",
    "    for process in processes:\n",
    "        process.join()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5d89935b22884bac8e846ef6a5fb14ff565b7e382ac92ebe1031401d4b8e3f29"
  },
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
