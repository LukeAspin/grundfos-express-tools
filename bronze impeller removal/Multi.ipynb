{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "from openpyxl import load_workbook,worksheet,Workbook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "MYDIR= r\"C:\\Projects\\2022\\Michaels_Code\\grundfos-express-tools\\bronze impeller removal\\input files\"\n",
    "OUTPUT_DIR=r\"C:\\Projects\\2022\\Michaels_Code\\grundfos-express-tools\\bronze impeller removal\\output files\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PNs to be Removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_pns = [\n",
    "    96699290, 97775274, 96699299, 97775277, 96778078,\n",
    "    97780992, 96699305, 96769184, 97778033, 96769190,\n",
    "    96769205, 97778039, 96769256, 96896891, 96769259,\n",
    "    96769271, 97780970, 96769280, 97780973\n",
    "]\n",
    "\n",
    "str_list = [str(numstr) for numstr in list_of_pns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Justification Note"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_removal_note():\n",
    "    timeStamp = datetime.datetime.now().strftime(\"%m-%d-%Y\")\n",
    "    reason = input(\"Reason for removal: \")\n",
    "    authority = input(\"Who authorized/requested this change? \")\n",
    "    return timeStamp + \" \" + reason + \" per \" + authority\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Geeting the First Row of the PSD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "wb=load_workbook(r'C:\\Projects\\2022\\Michaels_Code\\grundfos-express-tools\\bronze impeller removal\\input files\\Lbom-ES.xlsx',read_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_start_row(wb:Workbook,sheet_name:str,min_row:int=None,max_row:int=None,min_col:int=None,max_col:int=None):\n",
    "    try:\n",
    "        if not sheet_name in wb.sheetnames:\n",
    "            raise Exception\n",
    "        ws=wb[sheet_name]\n",
    "    except:\n",
    "        print('Sheet does not exist')\n",
    "    min_col = min_col or 1\n",
    "    min_row = min_row or 1\n",
    "    max_col = max_col or ws.max_column\n",
    "    max_row = max_row or ws.max_row\n",
    "    for row in ws.iter_rows(min_row,max_row,min_col,max_col):\n",
    "        if row[0].value=='[START]':\n",
    "            return row[0].row-1\n",
    "\n",
    "psd_startrow=get_start_row(wb,'Impeller',min_row=1,max_col=1)\n",
    "psd_startcol=0\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grouping Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_then_separate_by(psd_data:DataFrame,list_of_cols: list, pn_col: str)->tuple[DataFrame,DataFrame]:\n",
    "    \"\"\"list of cols are grouping categories. pn_col is the column that contains pns to find matches on.\"\"\"\n",
    "    groups = psd_data.groupby(list_of_cols) # Had to play around with this to get the right groupings\n",
    "\n",
    "    df_list_to_remove = [] # will hold list of dataframes to be removed. Will concatenate to 1 dataframe later\n",
    "    df_list_to_keep   = [] # will hold list of dataframes to remain in PSD. Will concatenate to 1 dataframe later\n",
    "\n",
    "    # Iterates through each sub-group, checking if there is a pn that meets criteria for removal\n",
    "    for _, frame in groups:\n",
    "        if any(frame[pn_col].isin(str_list)):   # Finding matching partnumbers to remove\n",
    "            df_list_to_remove.append(frame)     # Need to add this sub-group to a \"removed dataframe\"\n",
    "        else:\n",
    "            df_list_to_keep.append(frame)       # Need to retain this sub-group, add to a \"keep dataframe\"\n",
    "\n",
    "    # Concatenating list of dfs to single dfs.\n",
    "    removals = pd.concat(df_list_to_remove)\n",
    "    keep = pd.concat(df_list_to_keep)\n",
    "        \n",
    "    return removals, keep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterate Through Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening file for updates: Lbom-ES.xlsx\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from typing import Callable,Any,ParamSpec,TypeVar,Concatenate\n",
    "from _utils.file_ops import read_files_in_dir\n",
    "from _utils.Dataframe_tools import get_df\n",
    "\n",
    "P = ParamSpec('P')\n",
    "\n",
    "######## START HERE #####\n",
    "#Workbook contains Sheets and each file contains only one workbook therefore each file contains multiple sheets\n",
    "#We want to make a list for any given file of the sheets we want to process \n",
    "#We can take a single sheet and process over it with the below function\n",
    "\n",
    "# def process_sheet(sheet):\n",
    "#     pass\n",
    "\n",
    "#If we want to process multiple sheets within a file we need to loop over an iterable of the sheets we want and we can define this as a sepertate function.\n",
    "\n",
    "def process_file(file,sheets):\n",
    "    for sheet in sheets:\n",
    "        process_sheet(sheet)\n",
    "\n",
    "#I need to process each of the files in my directory \n",
    "def process_dir(dir):\n",
    "    for file in dir:\n",
    "        process_file(file)\n",
    "\n",
    "files=read_files_in_dir(MYDIR)\n",
    "sheetname='Impeller'\n",
    "\n",
    "\n",
    "for file in files:\n",
    "    removal_note=add_removal_note()\n",
    "    data,fname=file\n",
    "    psd_data=pd.read_excel(data,sheet_name=sheetname,header=psd_startrow-1,dtype={'BOM':str})\n",
    "    original_psd_bottom_row = len(psd_data) + psd_startrow\n",
    "    \n",
    "    print(\"Opening file for updates: {}\".format(os.path.basename(fname)))\n",
    "    end_row = psd_data[psd_data['Full Data'] == '[END]'].index.to_list()[0]\n",
    "    psd_data = psd_data.iloc[:end_row]\n",
    "    group_by_columns = [\"Model\", \"Price ID\"]\n",
    "    removals, keep = group_then_separate_by(psd_data,group_by_columns, \"BOM\")\n",
    "    removals.loc[removals[\"Full Data\"] == \"[START]\", \"Full Data\"] = \"\"\n",
    "    new_row = pd.DataFrame({'ID': removal_note}, index =[0])\n",
    "    removals = pd.concat([new_row, removals[:]]).reset_index()\n",
    "    column_list = keep.columns\n",
    "    removals = removals[column_list]\n",
    "    removals.sort_values(by=['ID'], inplace=True)\n",
    "    keep.loc[0,'Full Data'] = np.nan\n",
    "    keep.sort_values(by=['ID'], inplace=True)\n",
    "    keep.reset_index(drop=True, inplace=True)\n",
    "    keep.loc[0,'Full Data'] = \"[START]\"\n",
    "    num_rows = len(keep)                      \n",
    "    keep.loc[num_rows,'Full Data']=\"[END]\"\n",
    "    append_location = original_psd_bottom_row - len(removals) \n",
    "    after_end_row = num_rows + psd_startrow + 2\n",
    "\n",
    "\n",
    "#Assuming only one sheet for right now\n",
    "# def before_change_data(dir,sheet_name:list[list[str]],is_same_note=True):\n",
    "#     files=read_files_in_dir(dir)\n",
    "#     for i,file in files:\n",
    "#         sheet=[]\n",
    "#         note=add_removal_note()\n",
    "#         for sheet in sheet_name[i]:\n",
    "#             sheet.append({'header':get_start_row(load_workbook(file[0],read_only=True),sheet_name=sheet,min_row=1,max_col=1)})\n",
    "#             if not is_same_note:\n",
    "#                 note=add_removal_note()\n",
    "            \n",
    "# before_change_data()\n",
    "\n",
    "\n",
    "# a=list(before_change_data(MYDIR))\n",
    "\n",
    "\n",
    "# def change_data(file:tuple[bytes,str,str],psd_data_func:Callable[Concatenate[P],DataFrame],psd_data_args:tuple[list,dict]):\n",
    "#     removal_note=add_removal_note()\n",
    "#     data,fname=file\n",
    "#     psd_data=psd_data_func(data,*psd_data_args[0]) #will have to be called as a list or all have to be the same\n",
    "#     original_psd_bottom_row = len(psd_data) + psd_startrow\n",
    "#     print(\"Opening file for updates: {}\".format(os.path.basename(fname)))\n",
    "#     #...This is where the real processing begins...\n",
    "#     end_row = psd_data[psd_data['Full Data'] == '[END]'].index.to_list()[0]\n",
    "#     psd_data = psd_data.iloc[:end_row]\n",
    "#     group_by_columns = [\"Model\", \"Price ID\"]\n",
    "#     removals, keep = group_then_separate_by(psd_data,group_by_columns, \"BOM\")\n",
    "#     removals.loc[removals[\"Full Data\"] == \"[START]\", \"Full Data\"] = np.nan\n",
    "#     new_row = pd.DataFrame({'ID': removal_note}, index =[0])\n",
    "#     removals = pd.concat([new_row, removals[:]]).reset_index()\n",
    "#     column_list = keep.columns\n",
    "#     removals = removals[column_list]\n",
    "#     removals.sort_values(by=['ID'], inplace=True)\n",
    "#     keep.loc[0,'Full Data'] = np.nan\n",
    "#     keep.sort_values(by=['ID'], inplace=True)\n",
    "#     keep.reset_index(drop=True, inplace=True)\n",
    "#     keep.loc[0,'Full Data'] = \"[START]\"\n",
    "#     return keep,removals,fname\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking for Misses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Didn't miss any partnumbers. Good to go\n"
     ]
    }
   ],
   "source": [
    "if (len(keep[keep['BOM'].isin(str_list)])) == 0:\n",
    "    print(\"Didn't miss any partnumbers. Good to go\")\n",
    "else:\n",
    "    print(\"For some reason, the following entries were missed\")\n",
    "    print(keep[keep['BOM'].isin(str_list)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Return a formatted file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openpyxl.styles import PatternFill\n",
    "\n",
    "outputPath=r'C:\\Projects\\2022\\Michaels_Code\\grundfos-express-tools\\bronze impeller removal\\output files\\Revised Lbom-ES.xlsx'\n",
    "\n",
    "wb = load_workbook(outputPath)\n",
    "ws = wb['Impeller']\n",
    "tabName = sheetname + \" No Bronze\"\n",
    "wb.copy_worksheet(ws).title = tabName\n",
    "ws_modified = wb[tabName]\n",
    "ws_modified.delete_rows(after_end_row, len(removals)-1)\n",
    "end_cell=ws.cell(row=after_end_row-1,column=1)\n",
    "end_cell.fill=PatternFill(fill_type='solid',start_color=\"FFCC99\",end_color=\"FFCC99\")\n",
    "for rows in ws_modified.iter_rows(min_row=append_location+2, max_row=append_location+2, min_col=1, max_col=40):\n",
    "    for cell in rows:\n",
    "        cell.fill = PatternFill(start_color=\"FF0000\", end_color=\"FF0000\", fill_type=\"solid\")\n",
    "wb.save(outputPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df=pd.read_excel(outputPath,'Impeller',header=None)\n",
    "# header=df.loc[:4,:]\n",
    "# rest=df.loc[5:,:]\n",
    "# rest.reset_index(drop=True, inplace=True)\n",
    "# rest.columns=rest.loc[0,:]\n",
    "# rest=rest.drop(0)\n",
    "from typing import overload,Literal,Sequence,Hashable,Iterable,Callable\n",
    "from pandas._typing import DtypeArg,StorageOptions\n",
    "from pandas.util._decorators import Appender\n",
    "\n",
    "\n",
    "outputPath=r'C:\\Projects\\2022\\Michaels_Code\\grundfos-express-tools\\bronze impeller removal\\output files\\Revised Lbom-ES.xlsx'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def _find_header_end(df:DataFrame)->int:\n",
    "    for row in df.itertuples():\n",
    "        if row[1]=='[START]':\n",
    "            return int(row[0])\n",
    "\n",
    "\n",
    "@overload\n",
    "def _get_df(\n",
    "    io,\n",
    "    # sheet name is str or int -> DataFrame\n",
    "    sheet_name: str | int,\n",
    "    header: int | Sequence[int] | None = ...,\n",
    "    names=...,\n",
    "    index_col: int | Sequence[int] | None = ...,\n",
    "    usecols=...,\n",
    "    squeeze: bool | None = ...,\n",
    "    dtype: DtypeArg | None = ...,\n",
    "    engine: Literal[\"xlrd\", \"openpyxl\", \"odf\", \"pyxlsb\"] | None = ...,\n",
    "    converters=...,\n",
    "    true_values: Iterable[Hashable] | None = ...,\n",
    "    false_values: Iterable[Hashable] | None = ...,\n",
    "    skiprows: Sequence[int] | int | Callable[[int], object] | None = ...,\n",
    "    nrows: int | None = ...,\n",
    "    na_values=...,\n",
    "    keep_default_na: bool = ...,\n",
    "    na_filter: bool = ...,\n",
    "    verbose: bool = ...,\n",
    "    parse_dates=...,\n",
    "    date_parser=...,\n",
    "    thousands: str | None = ...,\n",
    "    decimal: str = ...,\n",
    "    comment: str | None = ...,\n",
    "    skipfooter: int = ...,\n",
    "    convert_float: bool | None = ...,\n",
    "    mangle_dupe_cols: bool = ...,\n",
    "    storage_options: StorageOptions = ...,\n",
    ") -> tuple[DataFrame,DataFrame,int]:\n",
    "    ...\n",
    "\n",
    "\n",
    "@Appender(pd.read_excel.__doc__,join='\\n')\n",
    "def _get_df(*args,**kwargs):\n",
    "    \"\"\"This extends the pandas read_excel method for our uses.\n",
    "    \"\"\"\n",
    "    df=pd.read_excel(*args,**kwargs)\n",
    "    header_size=_find_header_end(df)\n",
    "    header=df.loc[:header_size-2,:]\n",
    "    psd_df=df.loc[header_size-1:,:]\n",
    "    psd_df.reset_index(drop=True,inplace=True)\n",
    "    psd_df.columns=psd_df.loc[0,:]\n",
    "    psd_df=psd_df.drop(0)\n",
    "    original_length=len(df)\n",
    "    return header,psd_df,original_length\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process Sheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from _types._types import FilePath\n",
    "from pandas._typing import FilePath, ReadBuffer\n",
    "from typing import Union\n",
    "from _utils.file_ops import read_files_in_dir\n",
    "\n",
    "files=read_files_in_dir(MYDIR)\n",
    "data,fname=next(files)\n",
    "\n",
    "def process_sheet(file: tuple[Union[FilePath, ReadBuffer[bytes], bytes],str], sheet_name: str | list[str],removal_note:str|None|bool):\n",
    "    \"\"\"\n",
    "    Params:\n",
    "    data: Data comes from the binary of the file or the file path. \n",
    "    sheet_name: The name of the sheet to process.\n",
    "    removal_note: Does the sheet have its own removal note or does it share one with the entire workbook (file). The default is set to it shares a note.\"\"\"\n",
    "    #Check if sheetname is a valid sheet in the file\n",
    "    data,fname=file\n",
    "    try:\n",
    "        if not sheet_name in pd.ExcelFile(data).sheet_names:\n",
    "            raise ValueError(f\"Worksheet named '{sheet_name}' not found\")\n",
    "    except ValueError as err:\n",
    "        print(err)\n",
    "    #Need to get the header dataframe and the psd_dataframe\n",
    "    header,psd_data,original_length=_get_df(data,sheet_name=sheet_name,header=None)\n",
    "    psd_data.astype({\"BOM\":str},copy=False)\n",
    "    if not bool(removal_note):\n",
    "        removal_note=add_removal_note()\n",
    "    return header\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making a New File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from _utils.excel_tools import write_excel_file\n",
    "from _utils.file_ops import add_str_to_filename\n",
    "from _utils.Dataframe_tools import write_df_to_excel\n",
    "\n",
    "header=process_sheet(file,'Impeller','No')\n",
    "tabName = sheetname + \" No Bronze\"\n",
    "\n",
    "write_df_to_excel(keep,outputPath,tabName,startrow=psd_startrow-1,startcol=psd_startcol)\n",
    "write_df_to_excel(removals,outputPath,tabName,header=False, startrow=append_location+1, startcol=psd_startcol)\n",
    "write_df_to_excel(header,outputPath,tabName,header=False, startrow=0, startcol=0)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5d89935b22884bac8e846ef6a5fb14ff565b7e382ac92ebe1031401d4b8e3f29"
  },
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
